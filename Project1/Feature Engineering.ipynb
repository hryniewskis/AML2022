{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-pension",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder ### available? Imho should be, like cmon\n",
    "from sklearn.model_selection import train_test_split ###hopefully we're allowed to use this too"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-deployment",
   "metadata": {},
   "source": [
    "# 1. Bank data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-limitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_df = pd.read_csv('data/bank-additional-full.csv', sep=';')\n",
    "bank_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-heavy",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-squad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in bank_df.columns:\n",
    "    if bank_df[c].dtype=='O':\n",
    "        print(bank_df[c].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-sterling",
   "metadata": {},
   "source": [
    "At first, it looks like there are no missing values in data. However, most off the categorical variables do have a special value `unknown` which is actually a missing value. While transforming the data by OHE, we can treat it like any other value or simply drop it to keep the columns linear independence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-symposium",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = bank_df['y']\n",
    "bank_df.drop('y', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-protection",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    \n",
    "    @staticmethod\n",
    "    def train_test_split(X, y, train_subset_proportion=0.75, keep_y_balance=True):\n",
    "        if set(X.index) != set(y.index):\n",
    "            raise AttributeError('Indices in X and y are not indetical')\n",
    "        n=X.shape[0]\n",
    "        train_rows_n = int(train_subset_proportion * n)\n",
    "        test_rows_n = n - train_rows_n \n",
    "        if keep_y_balance:\n",
    "            if ((y.unique()!=0) & (y.unique()!=1)).any():\n",
    "                raise ValueError('Using keep_y_balance requires y values to be 0 and 1.')\n",
    "            pos_index = y[y==1].index\n",
    "            neg_index = y[y==0].index\n",
    "            train_pos_index = np.random.choice(pos_index, int(train_subset_proportion*len(pos_index)), replace=False)\n",
    "            train_neg_index = np.random.choice(neg_index, int(train_subset_proportion*len(neg_index)), replace=False)\n",
    "            test_pos_index = np.array(list(set(pos_index) - set(train_pos_index)))\n",
    "            test_neg_index = np.array(list(set(neg_index) - set(train_neg_index)))\n",
    "            train_index = np.concatenate((train_pos_index, train_neg_index))\n",
    "            test_index = np.concatenate((test_pos_index, test_neg_index))\n",
    "        else:\n",
    "            train_index = np.random.choice(y.index, train_rows_n, replace=False)\n",
    "            test_index = np.array(set(y.index) - set(train_index))\n",
    "        return X.loc[train_index, :], X.loc[test_index, :], y.loc[train_index], y.loc[test_index]\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_multicollinearity(X):\n",
    "        \"\"\"\n",
    "        https://stackoverflow.com/questions/25676145/capturing-high-multi-collinearity-in-statsmodels\n",
    "        https://en.wikipedia.org/wiki/Multicollinearity#Detection\n",
    "        \"\"\"\n",
    "        X = X.copy()\n",
    "        while True:\n",
    "            corr_m = np.corrcoef(X)\n",
    "            eigenvalues, eigenvectors = np.linalg.eig(corr_m)\n",
    "            #TODO\n",
    "            print(eigenvalues)\n",
    "            break\n",
    "    \n",
    "    def one_hot_encoding(self):\n",
    "        #TODO\n",
    "        pass        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2, y1, y2 = Preprocessor.train_test_split(bank_df.drop(columns='y'), bank_df['y']=='yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reverse-albany",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-stanford",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722ab354",
   "metadata": {},
   "source": [
    "# 3. Breast Cancer Wisconsin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031f151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wdbc_df=pd.read_csv('data/wdbc.csv')\n",
    "\n",
    "wdbc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465aa873",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_wdbc=wdbc_df['diagnosis']==\"M\"\n",
    "X_wdbc=wdbc_df.drop(columns=[\"id\",\"diagnosis\",\"Unnamed: 32\"])\n",
    "\n",
    "X_wdbc.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67c956f",
   "metadata": {},
   "source": [
    "As we can see all features are non-null numeric type variables. Which means that in this case one-hot-encoding won't be needed. The only things left to do is to remove collinear and multicollinear ones (maybe remove some outliers? from data) and split data into training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75487cb6",
   "metadata": {},
   "source": [
    "Correlation matrix showing that we should probably remove a fair number of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6757e9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,13))\n",
    "sns.heatmap(X_wdbc.corr())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ffdc56",
   "metadata": {},
   "source": [
    "Removal of variables based only on correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf96182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeleteCorrelated(X,thresh=0.75):\n",
    "    X=X.copy()\n",
    "    cor_matrix = X.corr().abs()\n",
    "    upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(bool))\n",
    "    to_drop = [column for column in upper_tri.columns if any(upper_tri[column] >= thresh)]\n",
    "    X_cleaned=X.drop(columns=to_drop)\n",
    "    return X_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb9906c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wdbc_cleaned_corr=DeleteCorrelated(X_wdbc,0.8)\n",
    "X_wdbc_cleaned_corr.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a23d06a",
   "metadata": {},
   "source": [
    "Removal of variables using Variance Inflation Factor (VIF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d017364",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#source: https://www.kaggle.com/remilpm/how-to-remove-multicollinearity\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "class ReduceVIF(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, thresh=5, impute=True, impute_strategy='median'):\n",
    "        # From looking at documentation, values between 5 and 10 are \"okay\".\n",
    "        # Above 10 is too high and so should be removed.\n",
    "        self.thresh = thresh\n",
    "        \n",
    "        # The statsmodel function will fail with NaN values, as such we have to impute them.\n",
    "        # By default we impute using the median value.\n",
    "        # This imputation could be taken out and added as part of an sklearn Pipeline.\n",
    "        if impute:\n",
    "            self.imputer = SimpleImputer(strategy=impute_strategy)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print('ReduceVIF fit')\n",
    "        if hasattr(self, 'imputer'):\n",
    "            self.imputer.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print('ReduceVIF transform')\n",
    "        columns = X.columns.tolist()\n",
    "        if hasattr(self, 'imputer'):\n",
    "            X = pd.DataFrame(self.imputer.transform(X), columns=columns)\n",
    "        return ReduceVIF.calculate_vif(X, self.thresh)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_vif(X, thresh):\n",
    "        dropped=True\n",
    "        while dropped:\n",
    "            variables = X.columns\n",
    "            dropped = False\n",
    "            vif = [variance_inflation_factor(X[variables].values, X.columns.get_loc(var)) for var in X.columns]\n",
    "            \n",
    "            max_vif = max(vif)\n",
    "            if max_vif > thresh:\n",
    "                maxloc = vif.index(max_vif)\n",
    "                print(f'Dropping {X.columns[maxloc]} with vif={max_vif}')\n",
    "                X = X.drop([X.columns.tolist()[maxloc]], axis=1)\n",
    "                dropped=True\n",
    "        print(X.shape[1],\" features left in dataset\")\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03d092f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mult_Coll = ReduceVIF()\n",
    "X_wdbc_cleaned = Mult_Coll.fit_transform(X_wdbc)\n",
    "X_wdbc_cleaned.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1f4986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_wdbc_cleaned, y_wdbc)\n",
    "model.score(X_wdbc_cleaned, y_wdbc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479426d6",
   "metadata": {},
   "source": [
    "## 4. Etherneum frauds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a184a0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "etherneum_df=pd.read_csv('data/transaction_dataset.csv')\n",
    "\n",
    "etherneum_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275090bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for c in etherneum_df.columns:\n",
    "    if(len(etherneum_df[c].unique())<10):\n",
    "        print(c,etherneum_df[c].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa46c6ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "etherneum_df[' ERC20 uniq sent addr.1'].fillna(0)\n",
    "etherneum_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29e3538",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop=['Unnamed: 0',\n",
    "         'Index',\n",
    "         'Address',\n",
    "         ' ERC20 avg time between sent tnx',\n",
    "         ' ERC20 avg time between rec tnx',\n",
    "         ' ERC20 avg time between rec 2 tnx',\n",
    "         ' ERC20 avg time between contract tnx',\n",
    "         ' ERC20 min val sent contract',\n",
    "         ' ERC20 max val sent contract',\n",
    "         ' ERC20 avg val sent contract']\n",
    "etherneum_df.drop(columns=to_drop,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e821a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for c in etherneum_df.columns:\n",
    "    i+=1\n",
    "    if etherneum_df[c].dtype=='O':\n",
    "        print(i,c,len(etherneum_df[c].unique()))\n",
    "#todo onehotencoding them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a867c9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_eth=etherneum_df['FLAG']\n",
    "X_eth=etherneum_df.drop(columns='FLAG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52b4e90",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,13))\n",
    "sns.heatmap(X_eth.corr())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a019b94a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Mult_Coll = ReduceVIF(thresh=7)\n",
    "X_eth_cleaned = Mult_Coll.fit_transform(X_eth.drop(columns=[' ERC20_most_rec_token_type',' ERC20 most sent token type']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69d4df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,13))\n",
    "sns.heatmap(X_eth_cleaned.corr())\n",
    "plt.show()\n",
    "#one could still consider droping some of the variables because of high correlation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
